{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/wt9pyx6g40tookoymF57"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"G4YAUPEFiJ3N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91d82b9a-e186-48c7-f122-dfb62f9f7c75"},"outputs":[{"output_type":"stream","name":"stdout","text":[">>> PART 1: Setting up environment...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n",">>> PART 2: Preparing data...\n","Data preparation complete.\n","\n",">>> PART 3: Loading LSTM benchmark model...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["LSTM model loaded successfully.\n","\n",">>> PART 4: Rebuilding and retraining the Transformer model...\n","Starting Transformer model training... Please wait.\n","Epoch 1/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - loss: 0.3773\n","Epoch 2/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0440\n","Epoch 3/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0339\n","Epoch 4/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - loss: 0.0351\n","Epoch 5/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0325\n","Epoch 6/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0320\n","Epoch 7/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.0339\n","Epoch 8/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0344\n","Epoch 9/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 0.0340\n","Epoch 10/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0310\n","Epoch 11/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 0.0194\n","Epoch 12/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0278\n","Epoch 13/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0371\n","Epoch 14/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0356\n","Epoch 15/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0311\n","Epoch 16/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0338\n","Epoch 17/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0323\n","Epoch 18/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - loss: 0.0323\n","Epoch 19/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0320\n","Epoch 20/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0307\n","Epoch 21/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0345\n","Epoch 22/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0325\n","Epoch 23/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0322\n","Epoch 24/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0315\n","Epoch 25/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0305\n","Epoch 26/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.0337\n","Epoch 27/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0319\n","Epoch 28/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0333\n","Epoch 29/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0329\n","Epoch 30/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0321\n","Epoch 31/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0304\n","Epoch 32/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0323\n","Epoch 33/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0322\n","Epoch 34/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 0.0323\n","Epoch 35/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0307\n","Epoch 36/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0315\n","Epoch 37/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0333\n","Epoch 38/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0340\n","Epoch 39/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - loss: 0.0328\n","Epoch 40/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0319\n","Epoch 41/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 0.0332\n","Epoch 42/50\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 0.0315\n","Epoch 43/50\n","\u001b[1m 9/29\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 1s/step - loss: 0.0350"]}],"source":["# ==============================================================================\n","# FINAL VERSION FOR NOTEBOOK 4: MODEL COMPARISON\n","# This script will load the LSTM model and retrain the Transformer model\n","# from scratch to bypass all loading errors.\n","# ==============================================================================\n","\n","# --- PART 1: SETUP AND IMPORTS ---\n","print(\">>> PART 1: Setting up environment...\")\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n","\n","# Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# --- PART 2: DATA PREPARATION (Same as before) ---\n","print(\"\\n>>> PART 2: Preparing data...\")\n","# Load data\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Thai_Quant_AI_Project/02_data/set50_processed_data.csv'\n","df = pd.read_csv(data_path, index_col='Date', parse_dates=True)\n","\n","# Scale and split data\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(df)\n","training_size = int(len(scaled_data) * 0.8)\n","time_step = 60\n","train_data = scaled_data[0:training_size, :]\n","test_data = scaled_data[training_size - time_step:, :]\n","\n","# Windowing function\n","def create_dataset(dataset, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(dataset) - time_step - 1):\n","        a = dataset[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(dataset[i + time_step, 3])\n","    return np.array(dataX), np.array(dataY)\n","\n","X_train, y_train = create_dataset(train_data, time_step)\n","X_test, y_test = create_dataset(test_data, time_step)\n","print(\"Data preparation complete.\")\n","\n","# --- PART 3: LOAD THE LSTM MODEL (This part works fine) ---\n","print(\"\\n>>> PART 3: Loading LSTM benchmark model...\")\n","lstm_model_path = '/content/drive/MyDrive/Colab Notebooks/Thai_Quant_AI_Project/03_models/lstm_benchmark_model.h5'\n","lstm_model = tf.keras.models.load_model(lstm_model_path)\n","print(\"LSTM model loaded successfully.\")\n","\n","# --- PART 4: REBUILD AND RETRAIN THE TRANSFORMER MODEL ---\n","print(\"\\n>>> PART 4: Rebuilding and retraining the Transformer model...\")\n","# Transformer architecture functions\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n","    x = Dropout(dropout)(x)\n","    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n","    ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(inputs.shape[-1])])\n","    x = ffn(x)\n","    x = Dropout(dropout)(x)\n","    x = LayerNormalization(epsilon=1e-6)(x + x)\n","    return x\n","\n","def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    positions = tf.range(start=0, limit=input_shape[0], delta=1)\n","    pos_embedding = tf.keras.layers.Embedding(input_dim=input_shape[0], output_dim=input_shape[1])(positions)\n","    x = x + pos_embedding\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = Dense(dim, activation=\"relu\")(x)\n","        x = Dropout(mlp_dropout)(x)\n","    outputs = Dense(1)(x)\n","    return Model(inputs, outputs)\n","\n","# Build a new Transformer model instance\n","transformer_model = build_transformer_model(\n","    X_train.shape[1:], head_size=256, num_heads=4, ff_dim=4,\n","    num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25\n",")\n","transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model (this will take 5-15 minutes)\n","print(\"Starting Transformer model training... Please wait.\")\n","transformer_model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1)\n","print(\"Transformer model training complete.\")\n","\n","# --- PART 5: GENERATE PREDICTIONS & COMPARE RESULTS ---\n","print(\"\\n>>> PART 5: Generating predictions and comparing results...\")\n","# Generate predictions\n","lstm_predictions = lstm_model.predict(X_test)\n","transformer_predictions = transformer_model.predict(X_test)\n","\n","# Inverse transform function\n","def inverse_transform_predictions(predictions, original_scaled_data):\n","    pred_full = np.zeros(shape=(len(predictions), original_scaled_data.shape[1]))\n","    pred_full[:, 3] = predictions.flatten()\n","    pred_inv = scaler.inverse_transform(pred_full)[:, 3]\n","    return pred_inv\n","\n","lstm_pred_inv = inverse_transform_predictions(lstm_predictions, scaled_data)\n","transformer_pred_inv = inverse_transform_predictions(transformer_predictions, scaled_data)\n","actual_prices = df['Close'][-len(y_test):].values\n","\n","# Quantitative Comparison (RMSE)\n","lstm_rmse = np.sqrt(mean_squared_error(actual_prices, lstm_pred_inv))\n","transformer_rmse = np.sqrt(mean_squared_error(actual_prices, transformer_pred_inv))\n","results_df = pd.DataFrame({\n","    'Model': ['LSTM (Benchmark)', 'Transformer (Upgrade)'],\n","    'Test RMSE (Baht)': [lstm_rmse, transformer_rmse]\n","})\n","results_df['Improvement'] = ['-', f'{( (lstm_rmse - transformer_rmse) / lstm_rmse) * 100:.2f}%']\n","print(\"\\n--- Performance Comparison ---\")\n","print(results_df)\n","\n","# Qualitative Comparison (Plot)\n","print(\"\\nGenerating comparison plot...\")\n","plt.style.use('seaborn-v0_8-whitegrid')\n","plt.figure(figsize=(18, 9))\n","plt.title('Model Comparison: Actual Price vs. Predictions', fontsize=18)\n","plt.xlabel('Date', fontsize=14)\n","plt.ylabel('SET50 Close Price (Baht)', fontsize=14)\n","test_dates = df.index[-len(actual_prices):]\n","plt.plot(test_dates, actual_prices, label='Actual Price', color='black', linewidth=2)\n","plt.plot(test_dates, lstm_pred_inv, label=f'LSTM Prediction (RMSE: {lstm_rmse:.2f})', color='dodgerblue', alpha=0.8)\n","plt.plot(test_dates, transformer_pred_inv, label=f'Transformer Prediction (RMSE: {transformer_rmse:.2f})', color='red', alpha=0.8)\n","plt.legend(fontsize=12)\n","plt.show()"]}]}